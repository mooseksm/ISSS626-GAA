{
  "hash": "6a6cc40aaa081cdf4ef40135b99632af",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-class Exercise 5\"\nauthor: \"Kock Si Min\"\ndate: September 23, 2024\ndate-modified: \"last-modified\"\ntoc: true\nexecute:\n  eval: true\n  echo: true\n  freeze: true\n  warning: false\n  message: false\n---\n\n\n# Lecture 5 notes and In-class Exercise 5\n\n::: callout-note\n## Advice from Prof Kam on Take-home Ex01:\n\n-   there is a highway at the top of the 6 provinces - should have created a \\~50m buffer around the 6 provinces then use the buffered area to clip the road lines in Bangkok Metropolitan Region\n\n-   should sieve out short road lengths from road lines data\n\n-   always plot data out, examine it in relation to the context to know whether data is properly prepared\n:::\n\n## 5.1 Overview\n\nIn this hands-on exercise, we will compute Global Measures of Spatial Autocorrelation (GMSA) using **spdep** package. The learning points of this hands-on exercise are to:\n\n-   import geospatial data using appropriate function(s) of **sf** package,\n\n-   import csv file using appropriate function of **readr** package,\n\n-   perform relational join using appropriate join function of **dplyr** package,\n\n-   compute Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of **spdep** package,\n\n    -   plot Moran scatterplot,\n\n    -   compute and plot spatial correlogram using appropriate function of **spdep** package.\n\n-   provide statistically correct interpretation of GSA statistics.\n\n## 5.2 Getting Started\n\n### 5.2.1 The analytical question\n\nIn spatial policy, one of the main development objective of the local government and planners is to ensure equal distribution of development in the province. Our task in this study, hence, is to apply appropriate spatial statistical methods to discover if development are even distributed geographically. If the answer is **No**. Then, our next question will be “is there sign of spatial clustering?”. And, if the answer for this question is **yes**, then our next question will be “where are these clusters?”\n\nIn this case study, we are interested to examine the spatial pattern of a selected development indicator (i.e. GDP per capita) of [Hunan Provice](https://en.wikipedia.org/wiki/Hunan), People Republic of China.\n\n### 5.2.2 The Study Area and Data\n\nTwo data sets will be used in this hands-on exercise, they are:\n\n-   Hunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format.\n\n-   Hunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n### 5.2.3 Setting the Analytical Tools\n\nBefore we get started, we need to ensure that **spdep**, **sf**, **tmap** and **tidyverse** packages of R are installed in R.\n\n-   **sf** is use for importing and handling geospatial data in R,\n\n-   **tidyverse** is mainly use for wrangling attribute data in R,\n\n-   **spdep** will be used to compute spatial weights, global and local spatial autocorrelation statistics, and\n\n-   **tmap** will be used to prepare cartographic quality chropleth map.\n\nThe code chunk below is used to perform the following tasks:\n\n-   creating a package list containing the necessary R packages,\n\n-   checking if the R packages in the package list have been installed in R,\n\n    -   if they have yet to be installed, RStudio will installed the missing packages,\n\n-   launching the packages into R environment.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf,tidyverse,spdep,tmap,sfdep)\n```\n:::\n\n\n## 5.3 Getting the Data into R environment\n\nIn this section, we will bring a geospatial data and its associated attribute table into R environment. The geospatial data is in ESRI shapefile format and the attribute table is in csv fomat.\n\n### 5.3.1 Import shapefile into R environment\n\nThe code chunk below uses [`st_read()`](https://r-spatial.github.io/sf/reference/st_read.html) of **sf** package to import Hunan shapefile into R. The imported shapefile will be a **simple features** **object** of **sf**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan <- st_read(dsn = \"data/geospatial\",\n                 layer = \"Hunan\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `Hunan' from data source \n  `C:\\mooseksm\\ISSS626-GAA\\In-class_Ex\\In-class_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n:::\n\n\nWe note that the simple features data has a polygon geometry and has 88 features and 7 fields. It is in WGS84 geographic coordinate system.\n\n### 5.3.2 Import csv file into R environment\n\nNext, we will import *Hunan_2012.csv* into R using [`read_csv()`](https://readr.tidyverse.org/reference/read_delim.html) of readr package. The output is in R data frame class.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n```\n:::\n\n\n### 5.3.3 Performing a relational join\n\nThe code chunk below will be used to update the attribute table of *hunan*’s SpatialPolygonsDataFrame with the attribute fields of *hunan2012* dataframe. This is performed by using [`left_join()`](#0) of **dplyr** package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhunan_GDPPC <- left_join(hunan,hunan2012)%>%\n  select(1:4,7,15)\n```\n:::\n\n\n### 5.3.4 Visualising Regional Development Indicator\n\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using `qtm()` of **tmap** package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\ntm_shape(hunan_GDPPC) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\",\n            main.title.size = 1,\n            legend.text.size = 0.6,\n            legend.height = 1.20,\n            legend.width = 1.20,\n            frame = TRUE)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n## 5.4 Global Measures of Spatial Association\n\n### 5.4.1 Deriving Queen's contiguity weights: sfdep methods\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wt = st_weights(nb,\n                         style = \"W\"),\n         .before = 1)\n```\n:::\n\n\n-   the way the code is written avoids having too many layers i.e. calculate nb first then use it to calculate weights\n\n-   use **sfdep** for this as its more handy than **spdep** for this use case; **sfdep also puts the outputs into the dataframe rather than printing it out in spdep** (with reference to [link](https://r4gdsa.netlify.app/chap08))\n\n-   The argument `style = \"W\"` indicates that the spatial weights will follow the \"row-standardized\" method, meaning that the weights for each region's neighbors sum to 1\n\n-   The `.before = 1` argument ensures that the new columns (`nb` and `wt`) will be inserted before the first column of the `hunan_GDPPC` dataset instead of being appended to last column\n\n-   contiguity matrix cannot work in zone without neighbours, will need to assign *allow_zone: TRUE*\n\n### 5.4.2 Computing Global Moran's I\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoranI <- global_moran(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n\nglimpse(moranI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n```\n\n\n:::\n:::\n\n\n-   above dont provide p-value hence we typical perform Moran's I test instead of just computing Moran's I statistics (next code below)\n\n### 5.4.3 Performing Global Moran's I test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_test(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMoran I test under randomisation\n\ndata:  x  \nweights: listw    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n```\n\n\n:::\n:::\n\n\n-   test for randomisation assumption\n\n-   p-value lesser than 0.05, reject null hypothesis at 95% confidence interval\n\n### 5.4.4 Performing Global Moran's I permutation test\n\nIn practice, Monte carlo simulation should be used to perform the statistical test. For **sfdep**, it is supported by *global_moran_perm()* - run nsim =99 aka 100 times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nglobal_moran_perm(wm_q$GDPPC,\n                       wm_q$nb,\n                       wm_q$wt,\n                  nsim=99)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n\n\n:::\n:::\n\n\n-   note that p-value is even smaller as there are more iterations - when put them together aka big number theory, converge better into normal distribution\n\n-   p-value lesser than 0.05, reject null hypothesis at 95% confidence interval\n\n-   can also run nsim = 999, no fixed number, and even at nsim = 999, results can be generated very fast. note that the results generated/conclusion attained is the same.\n\n### 5.4.5 Computing local Moran's I\n\nAfter determing presence of spatial autocorrelation using Global Moran's I, we now want to know where so we compute local Moran's I at county level:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa <- wm_q %>%\n  mutate(local_moran = local_moran(\n    GDPPC, nb, wt, nsim =99),\n    .before = 1) %>%\n  unnest(local_moran)\n```\n:::\n\n\n-   sfdep is a wrapper of spdep, hence besides generating the following fields, it places these columns into the data frame when the *unnest()* function is run:\n\n    -   Ii: the local Moran’s I statistics\n\n    -   E.Ii: the expectation of local moran statistic under the randomisation hypothesis\n\n    -   Var.Ii: the variance of local moran statistic under the randomisation hypothesis\n\n    -   Z.Ii:the standard deviate of local moran statistic\n\n    -   Pr(): the p-value of local moran statistic\n\n-   *unnest()* helps to put the values from a list into a table format\n\n-   note that there are 3 different p values i.e. *p_ii, p_ii_sim, p_folded_sim (use pysal) -* **tend to pick up p_ii_sim**\n\n-   note that sfdep also already provides the label for the different cluster/outlier types (\"mean\"/\"median\"/\"pysal\") but in spdep, this part is manual:\n\n![](images/clipboard-3326055164.png){width=\"539\"}\n\n-   note that different methods can give different results. note that when data is skewed, advise to use \"median\"\n\n### 5.4.6 Visualising local Moran's I\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\ntm_shape(lisa)+\n  tm_fill(\"ii\")+\n  tm_borders(alpha = 0.5)+\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 1)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n### 5.4.7 Visualising p-value of local Moran's I\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\ntm_shape(lisa)+\n  tm_fill(\"p_ii_sim\")+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 1)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nFor better comparison, we put them next to each other:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\nmap1 <- tm_shape(lisa)+\n  tm_fill(\"ii\")+\n  tm_borders(alpha = 0.5)+\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 1)\n\nmap2 <- tm_shape(lisa)+\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0,0.001,0.01,0.05,1),\n          labels = c(\"0.001\",\"0.01\",\"0.05\",\"Not sig\"))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 1)\n\ntmap_arrange(map1,map2,ncol = 2)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n## 5.5 LISA Map\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlisa_sig <- lisa %>%\n  filter(p_ii < 0.05)\ntmap_mode('plot')\ntm_shape(lisa)+\n  tm_polygons()+\n  tm_borders(alpha = 0.5)+\n  tm_shape(lisa_sig)+\n  tm_fill(\"mean\")+\n  tm_borders(alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n-   note from the p-values in *lisa* that some are not significant, hence the code above filters *p_ii* that are significant i.e. \\<0.05 for 95% confidence interval hence you ensure that whatever is on the map is a statistically significant cluster\n\nComparing against the local Moran's I and p-value maps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\nmap1 <- tm_shape(lisa)+\n  tm_fill(\"ii\")+\n  tm_borders(alpha = 0.5)+\n  tm_view(set.zoom.limits = c(6,8))+\n  tm_layout(main.title = \"local Moran's I of GDPPC\",\n            main.title.size = 1)\n\nmap2 <- tm_shape(lisa)+\n  tm_fill(\"p_ii_sim\",\n          breaks = c(0,0.001,0.01,0.05,1),\n          labels = c(\"0.001\",\"0.01\",\"0.05\",\"Not sig\"))+\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"p-value of local Moran's I\",\n            main.title.size = 1)\n\nmap3 <- tm_shape(lisa)+\n  tm_polygons()+\n  tm_borders(alpha = 0.5)+\n  tm_shape(lisa_sig)+\n  tm_fill(\"mean\")+\n  tm_borders(alpha = 0.4)+\n  tm_layout(main.title = \"LISA Map\",\n            main.title.size = 1)\n\ntmap_arrange(map1,map2,map3,ncol = 3)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n-   note from the above that on the left, actually are all poor regions aka low but then only 1 region is flagged as statistically significant low-low\n\n## 5.6 Computing local Gi\\* statistics\n\nWe will need to\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_idw <- hunan_GDPPC %>%\n  mutate(nb = st_contiguity(geometry),\n         wts = st_inverse_distance(nb,geometry,\n                                   scale =1,\n                                   alpha =1),\n         .before = 1)\n```\n:::\n\n\nWe will compute local Gi\\* using the code chunk below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA <- wm_idw %>%\n  mutate(local_Gi = local_gstar_perm(\n    GDPPC,nb,wt,nsim=99),\n    .before = 1) %>%\n  unnest(local_Gi)\nHCSA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 88 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n# A tibble: 88 × 19\n   gi_star cluster   e_gi     var_gi std_dev p_value p_sim p_folded_sim skewness\n     <dbl> <fct>    <dbl>      <dbl>   <dbl>   <dbl> <dbl>        <dbl>    <dbl>\n 1  0.0416 Low     0.0114 0.00000641  0.0493 9.61e-1  0.7          0.35    0.875\n 2 -0.333  Low     0.0106 0.00000384 -0.0941 9.25e-1  1            0.5     0.661\n 3  0.281  High    0.0126 0.00000751 -0.151  8.80e-1  0.9          0.45    0.640\n 4  0.411  High    0.0118 0.00000922  0.264  7.92e-1  0.6          0.3     0.853\n 5  0.387  High    0.0115 0.00000956  0.339  7.34e-1  0.62         0.31    1.07 \n 6 -0.368  High    0.0118 0.00000591 -0.583  5.60e-1  0.72         0.36    0.594\n 7  3.56   High    0.0151 0.00000731  2.61   9.01e-3  0.06         0.03    1.09 \n 8  2.52   High    0.0136 0.00000614  1.49   1.35e-1  0.2          0.1     1.12 \n 9  4.56   High    0.0144 0.00000584  3.53   4.17e-4  0.04         0.02    1.23 \n10  1.16   Low     0.0104 0.00000370  1.82   6.86e-2  0.12         0.06    0.416\n# ℹ 78 more rows\n# ℹ 10 more variables: kurtosis <dbl>, nb <nb>, wts <list>, NAME_2 <chr>,\n#   ID_3 <int>, NAME_3 <chr>, ENGTYPE_3 <chr>, County <chr>, GDPPC <dbl>,\n#   geometry <POLYGON [°]>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode('plot')\ntm_shape(HCSA)+\n  tm_fill(\"gi_star\")+\n  tm_borders(alpha=0.5)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n## 5.7 Visualising hot spot and cold spot areas\n\nWe plot significant hot spot and cold spot areas using the appropriate tmap functions below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHCSA_sig <- HCSA %>%\n  filter(p_sim < 0.05)\ntmap_mode('plot')\ntm_shape(HCSA)+\n  tm_polygons()+\n  tm_borders(alpha = 0.5)+\n  tm_shape(HCSA_sig)+\n  tm_fill(\"cluster\")+\n  tm_borders(alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](In-class_Ex05_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n::: callout-note\n## Notes from Prof Kam:\n\n-   hands-on exercise 5 shows the old way to do it using spdep while the in-class exercise shows the new way to do it using sfdep\n:::\n\n::: callout-note\n## Notes from Prof Kam on Take-home Ex02:\n\n-   Thailand has very large tourism sector\n\n-   key indicators of tourism economy - should identify more than one i.e. besides total revenue by province\n\n-   emerging hot and cold spot areas will be covered in Lesson 6\n\n-   from take-home ex01, we already have the province level data (from HDX)\n:::\n",
    "supporting": [
      "In-class_Ex05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}